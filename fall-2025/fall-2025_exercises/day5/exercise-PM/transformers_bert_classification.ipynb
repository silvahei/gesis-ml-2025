{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URKn8K0HydhV"
      },
      "source": [
        "# Training and Fine-Tuning BERT for Classification\n",
        "## Classfying newspaper articles by topic\n",
        "\n",
        "This notebook will demonstrate how users can train and fine-tune a BERT model for classification with the popular HuggingFace `transformers` Python library.\n",
        "\n",
        "We will fine-tune a BERT model on news topics discussed [here](https://www.tandfonline.com/doi/full/10.1080/21670811.2020.1767509) with the goal of predicting the topic of a news article. The genres include:\n",
        "\n",
        "-   'business'\n",
        "-   'entertainment'\n",
        "-   'politics'\n",
        "-   'other'\n",
        "\n",
        "Please download the data from the [bdaca github](https://github.com/uvacw/teaching-bdaca/tree/main/modules/machinelearning-text-exercises)\n",
        "\n",
        "**Basic steps involved in using BERT and HuggingFace:**\n",
        "- Split your dataset into training, validation, and testing subsets.\n",
        "- Convert your data into a format that BERT can process.\n",
        "- Create dataset objects by joining your data and labels.\n",
        "- Load the pre-trained BERT model.\n",
        "- Refine the model by training it on your training data.\n",
        "- Use the model to make predictions and assess its performance on your test data.\n",
        "\n",
        "\n",
        "_This notebook is heavily inspired by Herties BERT for humanities tutorial_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y73xWUkpU1AC"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Import necessary Python libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNxmMnzoccfm",
        "outputId": "79e64fc2-5923-42ff-a770-b9f071c8b42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ9kmfrO3M_w"
      },
      "source": [
        "Next, we will import necessary Python libraries and modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9Si6kIWcULv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gzip\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import sys\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import ticker\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "sns.set(style='ticks', font_scale=1.2)\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import compute_sample_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVSwdfcY3YDa"
      },
      "outputs": [],
      "source": [
        "#datadir = '/Users/rupertkiddle/Desktop/teach/2024/Introduction to Machine Learning (GESIS)/3_datasets/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clone the repo into Colab's working directory\n",
        "!git clone https://github.com/uvacw/teaching-bdaca.git\n",
        "# move into the correct exercises folder\n",
        "%cd teaching-bdaca/modules/machinelearning-text-exercises/"
      ],
      "metadata": {
        "id": "p4lSSdKn33BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jp6IBa10_4R"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Read in data, and split into Train-Val-Test (60-20-20) samples**\n",
        "\n",
        "\n",
        "This will read in the annotated newspaper data from Vermeer et al., and split it into train, val and test samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3U_OBGb2S_E"
      },
      "outputs": [],
      "source": [
        "# In Google Colab: Add the dataset to your Google Drive\n",
        "# Run this cell to connect Colab to your drive.\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#os.chdir('/content/drive/MyDrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGHwaSDO0_4R"
      },
      "outputs": [],
      "source": [
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "def get_labeled_data(fn='labeled.csv'):\n",
        "    text= []\n",
        "    label= []\n",
        "\n",
        "    with open(fn) as fi:\n",
        "        next(fi)\n",
        "        reader = csv.reader(fi, delimiter=',')\n",
        "        for row in reader:\n",
        "            try:\n",
        "                text.append(row[0])\n",
        "                label.append(row[1])\n",
        "            except:\n",
        "                # invalid row, probably an empty one. let's just ignore\n",
        "                pass\n",
        "    return text, label\n",
        "texts, labels = get_labeled_data()\n",
        "\n",
        "\n",
        "# Split your data into training and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split your training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hix1EtOS0_4R"
      },
      "outputs": [],
      "source": [
        "print(f\"We have {len(X_train)} train examples, {len(X_val)} validation examples, and {len(X_test)} test examples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-34oep8LNKw"
      },
      "source": [
        "Here's an example of a training text and training label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcxvqUsdf5Sm"
      },
      "outputs": [],
      "source": [
        "X_train[0], y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FoaXKbKjXRX"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Implementing a Baseline Model using Logistic Regression**\n",
        "\n",
        "In this step, we train and evaluate a basic TF-IDF baseline model with logistic regression. Despite using a very small dataset, we observe a performance that is better than random. We will now check if BERT can outperform this strong baseline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBTyTh8Ui2D3"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "Xtrain = vectorizer.fit_transform(X_train)\n",
        "Xtest = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMjmweu6MIQU"
      },
      "source": [
        "We train a logistic regression model from scikit-learn on the newspaper training data, and then we use the trained model to make predictions on our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R92S7JZfjiaC"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(max_iter=1000).fit(Xtrain, y_train)\n",
        "predictions = model.predict(Xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6YmW7WZMhh3"
      },
      "source": [
        "We can leverage the `classification_report` function provided by scikit-learn to assess the performance of the logistic regression model in terms of its ability to predict newspaper topics that match the actual labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeJd8ogKjpg0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJZd6VNJ0_4U"
      },
      "source": [
        "What do you think of this model? Not too bad for a baseline model, right? Lets see whether we can improve this using BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aow3FPpppZVE"
      },
      "source": [
        "## Encode data for BERT\n",
        "\n",
        "To prepare our data for use with BERT, we need to encode the texts and labels in a way that the model can understand. Here are the steps we'll follow:\n",
        "\n",
        "1. Convert the labels from strings to integers.\n",
        "\n",
        "2. Tokenize the texts, which involves breaking them up into individual words, and then convert the words into \"word pieces\" that can be matched with their corresponding embedding vectors.\n",
        "\n",
        "3. Truncate texts that are longer than 512 tokens, or pad texts that are shorter than 512 tokens with a special padding token.\n",
        "\n",
        "4. Add special tokens to the beginning and end of each document, including a start token, a separator between sentences, and a padding token as necessary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRs0dEIoUZtV"
      },
      "source": [
        "We will be using the `AutoTokenizer.from_pretrained()` module from HuggingFace library to encode our texts. This module will handle all the encoding for us, including breaking word tokens into word pieces, truncating to 512 tokens, and adding padding and special BERT tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmIhSN4n3YDc"
      },
      "outputs": [],
      "source": [
        "# Ideally, we want to run our code on CUDA (NVIDIA GPUs using the program management system) or MPS (Apple Silicon GPUs).\n",
        "import torch\n",
        "\n",
        "# Check if there is a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the CUDA GPU.\n",
        "    device_name = torch.device(\"cuda\")\n",
        "    print('There are %d CUDA GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the CUDA GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# Check if MPS is available...\n",
        "elif torch.backends.mps.is_available():\n",
        "    # Tell PyTorch to use the MPS GPU.\n",
        "    device_name = torch.device(\"mps\")\n",
        "    print('MPS is available.')\n",
        "    print('We will use the MPS GPU.')\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device_name = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-knVab0H3YDc"
      },
      "outputs": [],
      "source": [
        "#We will be using a Dutch model, as our data is Dutch-- specifically the '\"GroNLP/bert-base-dutch-cased\"' model. Check out Hugging Face's documentation for more information on the different BERT models.\n",
        "model_name = 'GroNLP/bert-base-dutch-cased'\n",
        "\n",
        "# We set the maximum number of tokens in each document to be 512, which is the maximum length for BERT models.\n",
        "max_length = 512\n",
        "\n",
        "# We define the directory where we'll save our trained model. You can choose any name for the directory.\n",
        "save_directory = 'my_trained_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BEvRqpGVMUD"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#model = RobertaForSequenceClassification.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj8X7B30UvSj"
      },
      "source": [
        "In this section, we will generate a mapping of our news topics to integer keys. We begin by extracting the unique labels from our dataset and create a dictionary that associates each label with an integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSuo8gktjsVR"
      },
      "outputs": [],
      "source": [
        "unique_labels = set(label for label in y_train)\n",
        "label2id = {label: id for id, label in enumerate(unique_labels)}\n",
        "id2label = {id: label for label, id in label2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_iAWMtBpfhj"
      },
      "outputs": [],
      "source": [
        "label2id.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vle8EgkelwRa"
      },
      "outputs": [],
      "source": [
        "id2label.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EraNFBC8VnPu"
      },
      "source": [
        "Now let's encode our texts and labels!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDuGq_n4pgZX"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n",
        "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=max_length)\n",
        "test_encodings  = tokenizer(X_test, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "train_labels_encoded = [label2id[y] for y in y_train]\n",
        "val_labels_encoded = [label2id[y] for y in y_val]\n",
        "test_labels_encoded  = [label2id[y] for y in y_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X1sYEGsWDDh"
      },
      "source": [
        "**Examine a news article in the training set after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A89SN_ppiUP"
      },
      "outputs": [],
      "source": [
        "' '.join(train_encodings[0].tokens[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hvOn9RGWUMm"
      },
      "source": [
        "**Examine a news article in test set after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OafZQFKSwG9E"
      },
      "outputs": [],
      "source": [
        "' '.join(test_encodings[0].tokens[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jmt15FvW8FR"
      },
      "source": [
        "**Examine the training labels after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciemdVYwwMNz"
      },
      "outputs": [],
      "source": [
        "set(train_labels_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK1Ngb0wXBz9"
      },
      "source": [
        "**Examine the test labels after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TowwulYQwOff"
      },
      "outputs": [],
      "source": [
        "set(test_labels_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChcEv01TXI7v"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Create a custom Torch dataset by following these steps:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxWcyj0LXVtY"
      },
      "source": [
        "Here we combine the encoded labels and texts into dataset objects. We use the custom Torch `MyDataSet` class to make a `train_dataset` object from  the `train_encodings` and `train_labels_encoded`. We also make a `val_dataset`, `test_dataset` object from `test_encodings` and `val_encodings`, and `val_labels_encoded` and `test_labels_encoded`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4VCU-nepnqF"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyCFH3XEi4Ng"
      },
      "outputs": [],
      "source": [
        "train_dataset = MyDataset(train_encodings, train_labels_encoded)\n",
        "val_dataset = MyDataset(val_encodings, val_labels_encoded)\n",
        "test_dataset = MyDataset(test_encodings, test_labels_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSoXOmDYYyAK"
      },
      "source": [
        "**Examine a news article in the Torch `training_dataset` after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbJerEgC1Qpc"
      },
      "outputs": [],
      "source": [
        "' '.join(train_dataset.encodings[0].tokens[0:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq1M2Et4Y3LB"
      },
      "source": [
        "**Examine a news article in the Torch `test_dataset` after encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z65jnjVJ1aVB"
      },
      "outputs": [],
      "source": [
        "' '.join(test_dataset.encodings[1].tokens[0:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JJNlUuB0_4u"
      },
      "outputs": [],
      "source": [
        "len(id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkVgFcbCqKSu"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Initialize the pre-trained BERT model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2pSuFUVaDhP"
      },
      "source": [
        "We load a pre-trained Dutch BERT model and transfer it to CUDA for efficient computation.\n",
        "\n",
        "**Note**: If you intend to repeat the fine-tuning process after previously executing the subsequent cells, ensure that you re-run this cell to reload the original pre-trained model before commencing the fine-tuning again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7k75REXp7UJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(id2label)).to(device_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRQZuqcAqQNI"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Configure the parameters required for fine-tuning BERT**\n",
        "\n",
        "The following parameters are crucial for fine-tuning BERT and will be specified in the HuggingFace TrainingArguments objects that we will subsequently pass to the HuggingFace Trainer object. While there are numerous other arguments, we'll focus on the fundamental ones and some common pitfalls.\n",
        "\n",
        "When fine-tuning your own model, it's critical to experiment with these parameters to identify the optimal configuration for your specific dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYOaH9AhbCD_"
      },
      "source": [
        "| Parameter                     | Explanation                                                                                                                          |\n",
        "|-------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| `num_train_epochs`            | The total number of training epochs. This refers to how many times the entire dataset will be processed. Too many epochs can lead to overfitting.|\n",
        "| `per_device_train_batch_size` | The batch size per device during training.                                                                                           |\n",
        "| `per_device_eval_batch_size`  | The batch size for evaluation.                                                                                                      |\n",
        "| `warmup_steps`                | The number of warmup steps for the learning rate scheduler. A smaller value is recommended for small datasets.                         |\n",
        "| `weight_decay`                | The strength of weight decay, which reduces the size of weights, similar to regularization.                                          |\n",
        "| `output_dir`                  | The directory where the fine-tuned model and configuration files will be saved.                                                     |\n",
        "| `logging_dir`                 | The directory where logs will be stored.                                                                                            |\n",
        "| `logging_steps`               | How often to print logging output. This enables us to terminate training early if the loss is not decreasing.                        |\n",
        "| `evaluation_strategy`         | Evaluates while training so that we can monitor accuracy improvements.                                                              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pb3xtidn-HJ"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Fine-tune the BERT model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_SN_oGLV8Vw"
      },
      "source": [
        "Initially, we define a custom evaluation function that returns the accuracy of the model. However, this function can be modified to return other metrics such as precision, recall, F1 score, or any other desired evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNIt7fcnqUCp"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    labels = eval_pred.label_ids\n",
        "    preds = eval_pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    macro_f1 = f1_score(labels, preds, average='macro', sample_weight=compute_sample_weight('balanced', labels))\n",
        "    return {'accuracy': acc, 'macro_f1': macro_f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9xl5QLmWsAw"
      },
      "source": [
        "Then we create a HuggingFace `Trainer` object using the `TrainingArguments` object that we created above. We also send our `compute_metrics` function to the `Trainer` object, along with our test and train datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRyAR3o_0_4y"
      },
      "source": [
        "## **optimize your model based on a metric you select**\n",
        "Note: You can also use GridSearch to identify the optimal configuration. However, be aware that finetuning multiple times with different parameter combinations can be extremely resource-intensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7u4P7Sd0_4y"
      },
      "outputs": [],
      "source": [
        "metric_name = 'macro_f1' # you can chance this for `accuracy` etc, according to the function `compute_metrics`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5qQKs6k3YDi"
      },
      "outputs": [],
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_szoJJdX0_4y"
      },
      "outputs": [],
      "source": [
        "# Instantiate an object of the TrainingArguments class with the following parameters:\n",
        "training_args = TrainingArguments(\n",
        "\n",
        "    # Number of training epochs\n",
        "    num_train_epochs=5,\n",
        "\n",
        "    # Batch size for training\n",
        "    per_device_train_batch_size=8,\n",
        "\n",
        "    # Batch size for evaluation\n",
        "    per_device_eval_batch_size=8,\n",
        "\n",
        "    # Learning rate for optimization\n",
        "    learning_rate=5e-5,\n",
        "\n",
        "    # Load the best model at the end of training\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    # Metric used for selecting the best model\n",
        "    metric_for_best_model=metric_name,\n",
        "\n",
        "    # Number of warmup steps for the optimizer\n",
        "    warmup_steps=0,\n",
        "\n",
        "    # L2 regularization weight decay\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # Directory to save the fine-tuned model and configuration files\n",
        "    output_dir='./results',\n",
        "\n",
        "    # Directory to store logs\n",
        "    logging_dir='./logs',\n",
        "\n",
        "    # Log results every n steps\n",
        "    logging_steps=20,\n",
        "\n",
        "    # Strategy for evaluating the model during training\n",
        "    evaluation_strategy='steps',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgc8FS50qV0_"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,           # evaluation dataset (usually a validation set; here we just send our test set)\n",
        "    compute_metrics=compute_metrics      # our custom evaluation function\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo5QVLYjXGCN"
      },
      "source": [
        "Time to finally fine-tune!\n",
        "\n",
        "Be patient; if you've set everything in Colab to use GPUs, then it should only take a minute or two to run, but if you're running on CPU, it can take hours.\n",
        "\n",
        "After every 20 steps (as we specified in the TrainingArguments object), the trainer will output the current state of the model, including the training loss, validation loss, and accuracy (from our `compute_metrics` function).\n",
        "\n",
        "You should see the loss going down and the accuracy going up. If instead they are staying the same or oscillating, you probably need to change the fine-tuning parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W64JwriVqcmk"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXeIZ_LFqeps"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Save fine-tuned model**\n",
        "\n",
        "The following cell will save the model and its configuration files to a directory in Colab. To preserve this model for future use, you should download the model to your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxkDWDfvqeAo"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epiLftYkZrzc"
      },
      "source": [
        "(Optional) If you've already fine-tuned and saved the model, you can reload it using the following line. You don't have to run fine-tuning every time you want to evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A54QySLrO5I"
      },
      "outputs": [],
      "source": [
        "# trainer = AutoModelForSequenceClassification.from_pretrained(save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpzV4hFsLmZ6"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Evaluate fine-tuned model on the validation set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IvfhrBtYYcz"
      },
      "source": [
        "The following function of the `Trainer` object will run the built-in evaluation, including our `compute_metrics` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dshtTH0WLtM1"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daZGIdeN0_41"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Evaluate fine-tuned model on the test set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILJGLcCjYhPt"
      },
      "source": [
        "We may desire a more detailed evaluation of the model, hence we extract the predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_E8oVjeLuv2"
      },
      "outputs": [],
      "source": [
        "predicted_results = trainer.predict(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUYGfzczOuJE"
      },
      "outputs": [],
      "source": [
        "predicted_results.predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqUTa5irLyN8"
      },
      "outputs": [],
      "source": [
        "predicted_labels = predicted_results.predictions.argmax(-1) # Get the highest probability prediction\n",
        "predicted_labels = predicted_labels.flatten().tolist()      # Flatten the predictions into a 1D list\n",
        "predicted_labels = [id2label[l] for l in predicted_labels]  # Convert from integers back to strings for readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2jtqnJbPMpu"
      },
      "outputs": [],
      "source": [
        "len(predicted_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVcMU45fLzli"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,\n",
        "                            predicted_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddk-iKTQF-K3"
      },
      "source": [
        "<br><br>\n",
        "\n",
        "## **Extracting Correct and Incorrect Classifications for Analysis**\n",
        "\n",
        "Now that we have obtained the predicted labels, let's perform some analysis.\n",
        "\n",
        "The fine-tuning and extraction of predicted labels using BERT is now complete. You can use the predicted labels just like you would with any other classification model. Here are some examples.\n",
        "\n",
        "To start, let's print out some example predictions that were correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7KrvGuZkDAV"
      },
      "outputs": [],
      "source": [
        "for _true_label, _predicted_label, _text in random.sample(list(zip(y_test, predicted_labels, X_test)), 20):\n",
        "  if _true_label == _predicted_label:\n",
        "    print('LABEL:', _true_label)\n",
        "    print('REVIEW TEXT:', _text[:100], '...')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW30Z6ynkDPI"
      },
      "source": [
        "Now let's print out some misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmx1RSKDkIpG"
      },
      "outputs": [],
      "source": [
        "for _true_label, _predicted_label, _text in random.sample(list(zip(y_test, predicted_labels, X_test)), 80):\n",
        "  if _true_label != _predicted_label:\n",
        "    print('TRUE LABEL:', _true_label)\n",
        "    print('PREDICTED LABEL:', _predicted_label)\n",
        "    print('REVIEW TEXT:', _text[:100], '...')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MZqyFrckJBB"
      },
      "source": [
        "Finally, let's create some heatmaps to examine misclassification patterns. We could use these patterns to think about similarities and differences between genres, according to book reviewers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSAgS6tvivvz"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count the number of classifications for each genre pair\n",
        "genre_classifications = Counter(zip(y_test, predicted_labels))\n",
        "\n",
        "# Convert the counts to a DataFrame and pivot to wide format\n",
        "df_wide = pd.DataFrame(genre_classifications, index=['Number of Classifications']).T.reset_index()\n",
        "df_wide.columns = ['True Genre', 'Predicted Genre', 'Number of Classifications']\n",
        "df_wide = df_wide.pivot_table(index='True Genre', columns='Predicted Genre', values='Number of Classifications', fill_value=0)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(9,7))\n",
        "sns.set(style='ticks', font_scale=1.2)\n",
        "sns.heatmap(df_wide, linewidths=1, cmap='Purples')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiukJdYRqyjm"
      },
      "source": [
        "Looks good! We can see that overall, our model is assigning the correct labels for each genre.\n",
        "\n",
        "Now, let's remove the diagonal from the plot to highlight the misclassifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7GJQfYxi3ET",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "genre_classifications_dict = defaultdict(int)\n",
        "for _true_label, _predicted_label in zip(y_test, predicted_labels):\n",
        "  if _true_label != _predicted_label: # Remove the diagonal to highlight misclassifications\n",
        "    genre_classifications_dict[(_true_label, _predicted_label)] += 1\n",
        "\n",
        "dicts_to_plot = []\n",
        "for (_true_genre, _predicted_genre), _count in genre_classifications_dict.items():\n",
        "  dicts_to_plot.append({'True Genre': _true_genre,\n",
        "                        'Predicted Genre': _predicted_genre,\n",
        "                        'Number of Classifications': _count})\n",
        "\n",
        "df_to_plot = pd.DataFrame(dicts_to_plot)\n",
        "df_wide = df_to_plot.pivot_table(index='True Genre',\n",
        "                                 columns='Predicted Genre',\n",
        "                                 values='Number of Classifications')\n",
        "\n",
        "plt.figure(figsize=(9,7))\n",
        "sns.set(style='ticks', font_scale=1.2)\n",
        "sns.heatmap(df_wide, linewidths=1, cmap='Purples')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1feyS57A3YDk"
      },
      "source": [
        "### Bonus use the BERT model to get embeddings of our test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5iG1ROc3YDk"
      },
      "outputs": [],
      "source": [
        "# I use the pretrained model for this example -> you can also use your finetuned model here\n",
        "with torch.no_grad():\n",
        "    outputs = model(**test_encodings)\n",
        "    # outputs.last_hidden_state: (batch_size, seq_len, hidden_size)\n",
        "    # To get a single vector per text, use the [CLS] token embedding:\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :]  # shape: (batch_size, hidden_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gesis_iml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}